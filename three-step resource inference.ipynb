{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "import ast\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2, norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data and set thresholds (The only one cell need to be updated by the user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this package to store the test logs\n",
    "train_log_set_package_name = 'train_log_set'\n",
    "\n",
    "# use this package to store the test logs\n",
    "test_log_set_package_name = 'test_log_set'\n",
    "\n",
    "# use this package to store the outputs (to store the middle output after Step 2, and the final event logs with inferred resources after Step 3)\n",
    "generated_log_set_package_name = 'generated_log_set'\n",
    "\n",
    "# define the number of distinct resources to infer\n",
    "num_resources = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "timestamp_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "train_set_path = os.path.join(current_dir, train_log_set_package_name)\n",
    "train_logs_list = os.listdir(train_set_path)\n",
    "\n",
    "# create a dictionary to store train log dataframes\n",
    "df_dic_with_time_train = {}\n",
    "for train_log in train_logs_list:\n",
    "    log_name = train_log.replace(\".csv\", \"\")\n",
    "    train_log_path = os.path.join(train_set_path, train_log)\n",
    "    df_train = pd.read_csv(train_log_path)\n",
    "    df_train['timestamp'] = pd.to_datetime(df_train['timestamp'], utc=True)\n",
    "    df_train['timestamp'] = df_train['timestamp'].dt.tz_localize(None)\n",
    "    df_train['timestamp'] = df_train['timestamp'].dt.strftime(timestamp_format)\n",
    "    df_dic_with_time_train[log_name] = (df_train, timestamp_format)\n",
    "\n",
    "test_set_path = os.path.join(current_dir, test_log_set_package_name)\n",
    "test_logs_list = os.listdir(test_set_path)\n",
    "\n",
    "# create a dictionary to store train log dataframes\n",
    "df_dic_with_time_test = {}\n",
    "for test_log in test_logs_list:\n",
    "    log_name = test_log.replace(\".csv\", \"\")\n",
    "    test_log_path = os.path.join(test_set_path, test_log)\n",
    "    df_test = pd.read_csv(test_log_path)\n",
    "    df_test['timestamp'] = pd.to_datetime(df_test['timestamp'], utc=True)\n",
    "    df_test['timestamp'] = df_test['timestamp'].dt.tz_localize(None)\n",
    "    df_test['timestamp'] = df_test['timestamp'].dt.strftime(timestamp_format)\n",
    "    df_dic_with_time_test[log_name] = (df_test, timestamp_format)\n",
    "\n",
    "# create a package and record the path (to store the middle output after Step 2, and the final event logs with inferred resources after Step 3)\n",
    "package_path = os.path.join(current_dir, generated_log_set_package_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract basic knowledge from train logs (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is to extract resource, and activity knowledge from event logs\n",
    "\n",
    "# extract basic resource and handover knowledge from a single train log\n",
    "def get_case_resource_act_knowledge(mas_log):\n",
    "    case_knowledge = {}\n",
    "    for trace_id in range(len(mas_log)):\n",
    "        act_num, act_type_num, res_num, handover_num = 0, 0, 0, 0\n",
    "        act_l, res_l = [], []\n",
    "        trace_name = 'trace_' + str(trace_id)\n",
    "        current_trace = mas_log[trace_id]\n",
    "        for task_id in range(len(mas_log[trace_id])):\n",
    "            act_num += 1\n",
    "\n",
    "            if current_trace[task_id]['concept:name'] not in act_l:\n",
    "                act_type_num += 1\n",
    "                act_l.append(current_trace[task_id]['concept:name'])\n",
    "\n",
    "            if current_trace[task_id]['agent_id'] not in res_l:\n",
    "                res_num += 1\n",
    "                res_l.append(current_trace[task_id]['agent_id'])\n",
    "\n",
    "            if (task_id + 1) < len(mas_log[trace_id]):\n",
    "                if current_trace[task_id]['agent_id'] != current_trace[task_id+1]['agent_id']:\n",
    "                    handover_num += 1\n",
    "        case_knowledge[trace_name] = [act_num, act_type_num, res_num, handover_num]\n",
    "\n",
    "    return case_knowledge\n",
    "\n",
    "\n",
    "# extract basic activity and resource knowledge from a single train log\n",
    "def get_theory_one_information_from_log(mas_log):\n",
    "    # resource_action_dic = {R1:{T1:2,T2:4,...}, R2:[T2:1,T3:3,...]}\n",
    "    # action_resource_dic = {T1:[R1:1,R2:3,...], T2:[R3:4,R4:1,...]}\n",
    "    resource_action_dic = {}\n",
    "    action_resource_dic = {}\n",
    "    # different resources set\n",
    "    diff_resource_list = []\n",
    "    # different actions set\n",
    "    diff_task_list = []\n",
    "    whole_act_num = 0\n",
    "    for trace_id in range(len(mas_log)):\n",
    "        current_trace = mas_log[trace_id]\n",
    "        for task_id in range(len(mas_log[trace_id])):\n",
    "            whole_act_num += 1\n",
    "            current_resource = current_trace[task_id]['agent_id']\n",
    "            # current_task_name = current_trace[task_id]['activity_type']\n",
    "            current_task_name = current_trace[task_id]['concept:name']\n",
    "            if current_resource not in diff_resource_list:\n",
    "                diff_resource_list.append(current_resource)\n",
    "            if current_task_name not in diff_task_list:\n",
    "                diff_task_list.append(current_task_name)\n",
    "            if current_resource not in resource_action_dic.keys():\n",
    "                act_dic = {}\n",
    "                act_dic[current_task_name] = 1\n",
    "                resource_action_dic[current_resource] = act_dic\n",
    "            else:\n",
    "                if current_task_name not in resource_action_dic[current_resource].keys():\n",
    "                    resource_action_dic[current_resource][current_task_name] = 1\n",
    "                else:\n",
    "                    resource_action_dic[current_resource][current_task_name] += 1\n",
    "\n",
    "            if current_task_name not in action_resource_dic.keys():\n",
    "                res_dic = {}\n",
    "                res_dic[current_resource] = 1\n",
    "                action_resource_dic[current_task_name] = res_dic\n",
    "            else:\n",
    "                if current_resource not in action_resource_dic[current_task_name].keys():\n",
    "                    action_resource_dic[current_task_name][current_resource] = 1\n",
    "                else:\n",
    "                    action_resource_dic[current_task_name][current_resource] += 1\n",
    "\n",
    "    return resource_action_dic, action_resource_dic, diff_resource_list, diff_task_list, whole_act_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Extraction\n",
    "### Extract Domain 1: handover count frequency per case (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Extraction\n",
    "\n",
    "# extract the Domain 1 (D1: handover count frequency per case)\n",
    "def get_case_hand_knowledge(df_dic):\n",
    "    win_num_dic = {'Poisson': [], 'Exponential': [], 'Geometric': [], 'Constant': []}\n",
    "    num_zero_hand_case_percent = []\n",
    "    for df_name, df_data in df_dic.items():\n",
    "        df = df_data[0].rename(columns={\n",
    "            'case_id': 'case:concept:name',     # Replace with actual case column name\n",
    "            'activity_type': 'concept:name',  # Replace with actual activity column name\n",
    "            'timestamp': 'time:timestamp'  # Replace with actual timestamp column name\n",
    "        })\n",
    "        unclustered_log = log_converter.apply(df) \n",
    "        exp_1 = get_case_resource_act_knowledge(unclustered_log)\n",
    "\n",
    "        handover_num_l = []\n",
    "        for key, value in exp_1.items():\n",
    "            handover_num_l.append(value[3])\n",
    "        \n",
    "        data = handover_num_l\n",
    "        lambda_hat = np.mean(data)  # Poisson MLE\n",
    "        p_geom_hat = 1 / (np.mean(data) + 1)  # Geometric MLE\n",
    "        lambda_exp_hat = 1 / np.mean(data) \n",
    "        c_constant = np.mean(data)\n",
    "\n",
    "        values_c_dic = {}\n",
    "        values, counts = np.unique(data, return_counts=True)\n",
    "        for i in range(len(values)):\n",
    "            values_c_dic[values[i]] = counts[i]\n",
    "\n",
    "        counts_l = []\n",
    "        values_l = []\n",
    "        for j in range(max(values)):\n",
    "            if j in values_c_dic.keys():\n",
    "                values_l.append(j)\n",
    "                counts_l.append(values_c_dic[j])\n",
    "            else:\n",
    "                values_l.append(j)\n",
    "                counts_l.append(0)\n",
    "\n",
    "        counts_prob_l = counts_l / sum(counts_l)\n",
    "        num_zero_hand_case_percent.append(values_c_dic[0] / len(exp_1))\n",
    "\n",
    "        poisson_pred_l = stats.poisson.pmf(values_l, lambda_hat)\n",
    "        exp_pred_l = stats.expon.pdf(values_l, lambda_exp_hat)\n",
    "        geom_pred_l = stats.geom.pmf(values_l, p_geom_hat)\n",
    "        constant_pred_l = [1 / len(values_l)] * len(values_l)\n",
    "\n",
    "        def get_square_distance(l1, l2):\n",
    "            dist = 0\n",
    "            for i in range(len(l1)):\n",
    "                dist  = dist + (l1[i] - l2[i]) ** 2\n",
    "            return dist\n",
    "        \n",
    "        def get_poisson_CI(data):\n",
    "            lower = 0.5 * stats.chi2.ppf(0.05/2, 2*sum(data))\n",
    "            upper = 0.5 * stats.chi2.ppf(1 - 0.05/2, 2*sum(data) + 2)\n",
    "            ci = (lower / len(data), upper / len(data))\n",
    "            return ci\n",
    "        \n",
    "        def get_exp_CI(data):\n",
    "            x_bar = np.mean(data)\n",
    "            n = len(data)\n",
    "            lower = 2 * n * x_bar / chi2.ppf(1 - 0.05 / 2, df=2 * n)\n",
    "            upper = 2 * n * x_bar / chi2.ppf(0.05 / 2, df=2 * n)\n",
    "            ci = (1/upper, 1/lower)\n",
    "            return ci\n",
    "        \n",
    "        def get_geom_CI(data):\n",
    "            n = len(data)\n",
    "            x_bar = np.mean(data)\n",
    "            s = np.std(data, ddof=1)\n",
    "\n",
    "            # Estimate of p\n",
    "            p_hat = 1 / (x_bar + 1)\n",
    "\n",
    "            # Standard error using delta method\n",
    "            se_p_hat = s / (np.sqrt(n) * x_bar**2)\n",
    "\n",
    "            # Z-value for 95% confidence\n",
    "            z = norm.ppf(0.975)\n",
    "\n",
    "            # Confidence Interval\n",
    "            ci_lower = p_hat - z * se_p_hat\n",
    "            ci_upper = p_hat + z * se_p_hat\n",
    "            ci = (ci_lower, ci_upper)\n",
    "            return ci\n",
    "        \n",
    "        def get_cons_CI(data):\n",
    "            c_constant = np.mean(data)\n",
    "            # Confidence interval\n",
    "            n = len(data)\n",
    "            s = np.std(data, ddof=1)\n",
    "            z = norm.ppf(0.975)  # for 95% CI\n",
    "\n",
    "            margin_error = z * s / np.sqrt(n)\n",
    "            ci_lower = c_constant - margin_error\n",
    "            ci_upper = c_constant + margin_error\n",
    "            ci = (ci_lower, ci_upper)\n",
    "            return ci\n",
    "\n",
    "        pos_v = get_square_distance(counts_prob_l, poisson_pred_l)\n",
    "        exp_v = get_square_distance(counts_prob_l, exp_pred_l)\n",
    "        geom_v = get_square_distance(counts_prob_l, geom_pred_l)\n",
    "        const_v = get_square_distance(counts_prob_l, constant_pred_l)\n",
    "\n",
    "        distances = {'Poisson': (pos_v, lambda_hat, get_poisson_CI(data)), 'Exponential': (exp_v, lambda_exp_hat, get_exp_CI(data)), 'Geometric': (geom_v, p_geom_hat, get_geom_CI(data)), 'Constant': (const_v, c_constant, get_cons_CI(data))}\n",
    "        best_model = min(distances, key=lambda k: distances[k][0])\n",
    "        MLE_distance = distances[best_model][0]\n",
    "        param_estimate = distances[best_model][1]\n",
    "        ci = distances[best_model][2]\n",
    "        win_num_dic[best_model].append((df_name, MLE_distance, param_estimate, ci))\n",
    "    \n",
    "    # our strategy is to compare the win number of these distributions firstly, if multiple approaches have the same number, then we compare their sum(distance)\n",
    "    def get_possible_interval(win_num_dic):\n",
    "        methods_ci_dic = {}\n",
    "        for key, tups in win_num_dic.items():\n",
    "            if tups != []:\n",
    "                ci_left_l, ci_right_l = [], []\n",
    "                for tup in tups:\n",
    "                    ci_left_l.append(tup[3][0])\n",
    "                    ci_right_l.append(tup[3][1])\n",
    "                min_left = min(ci_left_l)\n",
    "                max_right = max(ci_right_l)\n",
    "                methods_ci_dic[key] = (min_left, max_right)\n",
    "        return methods_ci_dic\n",
    "    \n",
    "    def get_sum_distance(tup_l):\n",
    "        sum = 0\n",
    "        for tup in tup_l:\n",
    "            sum += tup[1]\n",
    "        return sum\n",
    "\n",
    "    win_max_num = max(len(v) for v in win_num_dic.values())\n",
    "    methods_ci_dic = get_possible_interval(win_num_dic)\n",
    "    win_distri_l = [k for k, v in win_num_dic.items() if len(v) == win_max_num]\n",
    "    if len(win_distri_l) == 1:\n",
    "        return [win_distri_l[0], methods_ci_dic[win_distri_l[0]], num_zero_hand_case_percent]\n",
    "    else:\n",
    "        best_method = win_distri_l[0]\n",
    "        min_dist_sum = get_sum_distance(win_num_dic[best_method])\n",
    "        for method in win_distri_l:\n",
    "            if get_sum_distance(win_num_dic[method]) <= min_dist_sum:\n",
    "                min_dist_sum = get_sum_distance(win_num_dic[method])\n",
    "                best_method = method\n",
    "        return [method, methods_ci_dic[method], num_zero_hand_case_percent]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Domain 2: distinct resource count frequency per case (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Extraction\n",
    "\n",
    "# extract the Domain 2 (D2: distinct resource count frequency per case)\n",
    "def get_case_resource_knowledge(df_dic):\n",
    "    win_num_dic = {'Poisson': [], 'Exponential': [], 'Logarithmic': [], 'Geometric': [], 'Constant': []}\n",
    "    num_zero_res_case_percent = []\n",
    "    for df_name, df_data in df_dic.items():\n",
    "        df = df_data[0].rename(columns={\n",
    "            'case_id': 'case:concept:name',     # Replace with actual case column name\n",
    "            'activity_type': 'concept:name',  # Replace with actual activity column name\n",
    "            'timestamp': 'time:timestamp'  # Replace with actual timestamp column name\n",
    "        })\n",
    "        unclustered_log = log_converter.apply(df) \n",
    "        exp_1 = get_case_resource_act_knowledge(unclustered_log)\n",
    "\n",
    "        res_num_l = []\n",
    "        for key, value in exp_1.items():\n",
    "            # for all the resource num, minus 1 to fill in these distributions\n",
    "            res_num_l.append(value[2]-1)\n",
    "        \n",
    "        data = res_num_l\n",
    "        lambda_hat = np.mean(data)  # Poisson MLE\n",
    "        p_geom_hat = 1 / (np.mean(data) + 1)  # Geometric MLE\n",
    "        lambda_exp_hat = 1 / np.mean(data) \n",
    "        c_constant = np.mean(data)\n",
    "\n",
    "        values_c_dic = {}\n",
    "        values, counts = np.unique(data, return_counts=True)\n",
    "        for i in range(len(values)):\n",
    "            values_c_dic[values[i]] = counts[i]\n",
    "\n",
    "        counts_l = []\n",
    "        values_l = []\n",
    "        for j in range(max(values)):\n",
    "            if j in values_c_dic.keys():\n",
    "                values_l.append(j)\n",
    "                counts_l.append(values_c_dic[j])\n",
    "            else:\n",
    "                values_l.append(j)\n",
    "                counts_l.append(0)\n",
    "\n",
    "        counts_prob_l = counts_l / sum(counts_l)\n",
    "        num_zero_res_case_percent.append(values_c_dic[0] / len(exp_1))\n",
    "\n",
    "        poisson_pred_l = stats.poisson.pmf(values_l, lambda_hat)\n",
    "        exp_pred_l = stats.expon.pdf(values_l, lambda_exp_hat)\n",
    "        geom_pred_l = stats.geom.pmf(values_l, p_geom_hat)\n",
    "        constant_pred_l = [1 / len(values_l)] * len(values_l)\n",
    "\n",
    "        def get_square_distance(l1, l2):\n",
    "            dist = 0\n",
    "            for i in range(len(l1)):\n",
    "                dist  = dist + (l1[i] - l2[i]) ** 2\n",
    "            return dist\n",
    "        \n",
    "        def get_poisson_CI(data):\n",
    "            lower = 0.5 * stats.chi2.ppf(0.05/2, 2*sum(data))\n",
    "            upper = 0.5 * stats.chi2.ppf(1 - 0.05/2, 2*sum(data) + 2)\n",
    "            ci = (lower / len(data), upper / len(data))\n",
    "            return ci\n",
    "        \n",
    "        def get_exp_CI(data):\n",
    "            x_bar = np.mean(data)\n",
    "            n = len(data)\n",
    "            lower = 2 * n * x_bar / chi2.ppf(1 - 0.05 / 2, df=2 * n)\n",
    "            upper = 2 * n * x_bar / chi2.ppf(0.05 / 2, df=2 * n)\n",
    "            ci = (1/upper, 1/lower)\n",
    "            return ci\n",
    "        \n",
    "        def get_geom_CI(data):\n",
    "            n = len(data)\n",
    "            x_bar = np.mean(data)\n",
    "            s = np.std(data, ddof=1)\n",
    "\n",
    "            # Estimate of p\n",
    "            p_hat = 1 / (x_bar + 1)\n",
    "\n",
    "            # Standard error using delta method\n",
    "            se_p_hat = s / (np.sqrt(n) * x_bar**2)\n",
    "\n",
    "            # Z-value for 95% confidence\n",
    "            z = norm.ppf(0.975)\n",
    "\n",
    "            # Confidence Interval\n",
    "            ci_lower = p_hat - z * se_p_hat\n",
    "            ci_upper = p_hat + z * se_p_hat\n",
    "            ci = (ci_lower, ci_upper)\n",
    "            return ci\n",
    "        \n",
    "        def get_cons_CI(data):\n",
    "            c_constant = np.mean(data)\n",
    "            # Confidence interval\n",
    "            n = len(data)\n",
    "            s = np.std(data, ddof=1)\n",
    "            z = norm.ppf(0.975)  # for 95% CI\n",
    "\n",
    "            margin_error = z * s / np.sqrt(n)\n",
    "            ci_lower = c_constant - margin_error\n",
    "            ci_upper = c_constant + margin_error\n",
    "            ci = (ci_lower, ci_upper)\n",
    "            return ci\n",
    "\n",
    "        pos_v = get_square_distance(counts_prob_l, poisson_pred_l)\n",
    "        exp_v = get_square_distance(counts_prob_l, exp_pred_l)\n",
    "        geom_v = get_square_distance(counts_prob_l, geom_pred_l)\n",
    "        const_v = get_square_distance(counts_prob_l, constant_pred_l)\n",
    "\n",
    "        distances = {'Poisson': (pos_v, lambda_hat, get_poisson_CI(data)), 'Exponential': (exp_v, lambda_exp_hat, get_exp_CI(data)), 'Geometric': (geom_v, p_geom_hat, get_geom_CI(data)), 'Constant': (const_v, c_constant, get_cons_CI(data))}\n",
    "        best_model = min(distances, key=lambda k: distances[k][0])\n",
    "        MLE_distance = distances[best_model][0]\n",
    "        param_estimate = distances[best_model][1]\n",
    "        ci = distances[best_model][2]\n",
    "        win_num_dic[best_model].append((df_name, MLE_distance, param_estimate, ci))\n",
    "    \n",
    "    # our strategy is to compare the win number of these distributions firstly, if multiple approaches have the same number, then we compare their sum(distance)\n",
    "    def get_possible_interval(win_num_dic):\n",
    "        methods_ci_dic = {}\n",
    "        for key, tups in win_num_dic.items():\n",
    "            if tups != []:\n",
    "                ci_left_l, ci_right_l = [], []\n",
    "                for tup in tups:\n",
    "                    ci_left_l.append(tup[3][0])\n",
    "                    ci_right_l.append(tup[3][1])\n",
    "                min_left = min(ci_left_l)\n",
    "                max_right = max(ci_right_l)\n",
    "                methods_ci_dic[key] = (min_left, max_right)\n",
    "        return methods_ci_dic\n",
    "    \n",
    "    def get_sum_distance(tup_l):\n",
    "        sum = 0\n",
    "        for tup in tup_l:\n",
    "            sum += tup[1]\n",
    "        return sum\n",
    "\n",
    "    win_max_num = max(len(v) for v in win_num_dic.values())\n",
    "    methods_ci_dic = get_possible_interval(win_num_dic)\n",
    "    win_distri_l = [k for k, v in win_num_dic.items() if len(v) == win_max_num]\n",
    "    if len(win_distri_l) == 1:\n",
    "        return [win_distri_l[0], methods_ci_dic[win_distri_l[0]], num_zero_res_case_percent]\n",
    "    else:\n",
    "        best_method = win_distri_l[0]\n",
    "        min_dist_sum = get_sum_distance(win_num_dic[best_method])\n",
    "        for method in win_distri_l:\n",
    "            if get_sum_distance(win_num_dic[method]) <= min_dist_sum:\n",
    "                min_dist_sum = get_sum_distance(win_num_dic[method])\n",
    "                best_method = method\n",
    "        return [method, methods_ci_dic[method], num_zero_res_case_percent]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Domain 3: event trigger frequency per resource (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Extraction\n",
    "\n",
    "# extract the Domain 3 (D3: event trigger frequency per resource)\n",
    "def extract_aver_resource_rank_activity_freq_without_plot(df_dic_with_time_train):\n",
    "    wx_l, wy_l = [], []\n",
    "    rank_1_to_20_dic = {}\n",
    "    for i in range(0, 20):\n",
    "        rank_1_to_20_dic[i] = []\n",
    "\n",
    "    for df_name, df in df_dic_with_time_train.items():\n",
    "        x_l, y_l = [], []\n",
    "        df1 = df[0].rename(columns={\n",
    "            'case_id': 'case:concept:name',     # Replace with actual case column name\n",
    "            'activity_type': 'concept:name',  # Replace with actual activity column name\n",
    "            'timestamp': 'time:timestamp'  # Replace with actual timestamp column name\n",
    "        })\n",
    "        unclustered_log = log_converter.apply(df1)\n",
    "        \n",
    "        resource_action_dic, action_resource_dic, diff_resource_list, diff_task_list, whole_act_num = get_theory_one_information_from_log(unclustered_log)\n",
    "        sorted_resource_action_dic_keys_l = sorted(resource_action_dic.keys(), key=lambda k: sum(resource_action_dic[k].values()), reverse=True)\n",
    "        rank = 0\n",
    "        for i in range(len(sorted_resource_action_dic_keys_l)):\n",
    "            if rank < 20:\n",
    "                x_l.append(rank)\n",
    "                wx_l.append(rank)\n",
    "                y_l.append(sum(resource_action_dic[sorted_resource_action_dic_keys_l[i]].values()) / whole_act_num)\n",
    "                wy_l.append(sum(resource_action_dic[sorted_resource_action_dic_keys_l[i]].values()) / whole_act_num)\n",
    "                rank_1_to_20_dic[rank].append(sum(resource_action_dic[sorted_resource_action_dic_keys_l[i]].values()) / whole_act_num)\n",
    "                rank += 1\n",
    "\n",
    "    aver_x_l = list(rank_1_to_20_dic.keys())\n",
    "    aver_y_l = []\n",
    "    for i in aver_x_l:\n",
    "        aver_y_l.append(sum(rank_1_to_20_dic[i]) / len(rank_1_to_20_dic[i]))\n",
    "\n",
    "    output_dic = {}\n",
    "    for i in range(len(aver_x_l)):\n",
    "        output_dic[aver_x_l[i]] = aver_y_l[i]\n",
    "\n",
    "    return output_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Domain 4: case involvement frequency per resource (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Extraction\n",
    "\n",
    "# extract the Domain 4 (D4: case involvement frequency per resource)\n",
    "def get_resource_involve_case_num_without_plot(df_dic_with_time_train):\n",
    "    output_dic = {} # output_dic = {'dataset':[100cases, 20cases, ...]}\n",
    "\n",
    "    rank_1_to_20_dic = {}\n",
    "    for i in range(0, 20):\n",
    "        rank_1_to_20_dic[i] = []\n",
    "\n",
    "    for df_name, df in df_dic_with_time_train.items():\n",
    "        x_l, y_l = [], []\n",
    "        df1 = df[0].rename(columns={\n",
    "            'case_id': 'case:concept:name',     # Replace with actual case column name\n",
    "            'activity_type': 'concept:name',  # Replace with actual activity column name\n",
    "            'timestamp': 'time:timestamp'  # Replace with actual timestamp column name\n",
    "        })\n",
    "        unclustered_log = log_converter.apply(df1)\n",
    "        resource_action_dic, action_resource_dic, diff_resource_list, diff_task_list, whole_act_num = get_theory_one_information_from_log(unclustered_log)\n",
    "        single_set_dic = {}\n",
    "        for res in diff_resource_list:\n",
    "            single_set_dic[res] = [0, 0]\n",
    "        for trace_id in range(len(unclustered_log)):\n",
    "            current_trace = unclustered_log[trace_id]\n",
    "            case_res_l = []\n",
    "            for task_id in range(len(unclustered_log[trace_id])):\n",
    "                current_res = current_trace[task_id]['agent_id']\n",
    "                single_set_dic[current_res][0] += 1\n",
    "                if current_res not in case_res_l:\n",
    "                    case_res_l.append(current_res)\n",
    "            for res in case_res_l:\n",
    "                single_set_dic[res][1] += 1\n",
    "\n",
    "        sort_single_set_dic = dict(sorted(single_set_dic.items(), key=lambda item: item[1][1], reverse=True))\n",
    "        rank = 0\n",
    "        for res, l in sort_single_set_dic.items():\n",
    "            if rank < 20:\n",
    "                x_l.append(rank)\n",
    "                y_l.append(l[1])\n",
    "            rank += 1\n",
    "\n",
    "        x_l_1 = [x for x in x_l]\n",
    "        y_l_1 = [y / sum(y_l) for y in y_l]\n",
    "        for j in range(len(x_l_1)):\n",
    "            rank_1_to_20_dic[x_l_1[j]].append(y_l_1[j])\n",
    "\n",
    "    aver_x_l = list(rank_1_to_20_dic.keys())\n",
    "    aver_y_l = []\n",
    "    for i in aver_x_l:\n",
    "        aver_y_l.append(sum(rank_1_to_20_dic[i]) / len(rank_1_to_20_dic[i]))\n",
    "\n",
    "    output_aver = {}\n",
    "    for i in range(len(aver_x_l)):\n",
    "        output_aver[aver_x_l[i]] = aver_y_l[i]\n",
    "\n",
    "    output_dic[df_name] = sort_single_set_dic\n",
    "\n",
    "    return output_aver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train logistic regression classifier 1 (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Extraction\n",
    "# logistic regression model 1 training (will be applied to detect the handover)\n",
    "\n",
    "# construct the training datasets using all logs from the train set\n",
    "def extract_df_for_train(dic_for_train):\n",
    "    case_num, handover_num = 0, 0\n",
    "    var_dic = {'case name':[], 'event location':[], 'activity same or not':[], 'time duration':[], 'continuous event number':[], 'handover or not':[]}\n",
    "    for df_name, df in dic_for_train.items():\n",
    "        df1 = df[0].rename(columns={\n",
    "            'case_id': 'case:concept:name',     # Replace with actual case column name\n",
    "            'activity_type': 'concept:name',  # Replace with actual activity column name\n",
    "            'timestamp': 'time:timestamp'  # Replace with actual timestamp column name\n",
    "        })\n",
    "        unclustered_log = log_converter.apply(df1)\n",
    "\n",
    "        # get the average time duration for activity pairs firstly\n",
    "        time_per_act_pair = {}\n",
    "        aver_time_per_act_pair = {}\n",
    "        case_num += len(unclustered_log)\n",
    "        for trace_id in range(len(unclustered_log)):\n",
    "            trace_name = 'trace_' + str(trace_id)\n",
    "            current_trace = unclustered_log[trace_id]\n",
    "            for task_id in range(len(unclustered_log[trace_id])-1):\n",
    "                current_time = datetime.strptime(current_trace[task_id]['time:timestamp'][:19], df[1])\n",
    "                next_time = datetime.strptime(current_trace[task_id+1]['time:timestamp'][:19], df[1])\n",
    "                current_act = current_trace[task_id]['concept:name']\n",
    "                next_act = current_trace[task_id+1]['concept:name']\n",
    "                if (current_act, next_act) not in time_per_act_pair.keys():\n",
    "                    time_per_act_pair[(current_act, next_act)] = [(next_time - current_time).total_seconds()]\n",
    "                else:\n",
    "                    time_per_act_pair[(current_act, next_act)].append((next_time - current_time).total_seconds())\n",
    "\n",
    "        for pair, time_l in time_per_act_pair.items():\n",
    "            aver_time_per_act_pair[pair] = sum(time_l) / len(time_l)\n",
    "\n",
    "        for trace_id in range(len(unclustered_log)):\n",
    "            trace_name = 'trace_' + str(trace_id)\n",
    "            current_trace = unclustered_log[trace_id]\n",
    "            conti_event_len = 1\n",
    "            for task_id in range(len(unclustered_log[trace_id])-1):\n",
    "                current_time = datetime.strptime(current_trace[task_id]['time:timestamp'][:19], df[1])\n",
    "                next_time = datetime.strptime(current_trace[task_id+1]['time:timestamp'][:19], df[1])\n",
    "                current_act = current_trace[task_id]['concept:name']\n",
    "                next_act = current_trace[task_id+1]['concept:name']\n",
    "                current_res = current_trace[task_id]['agent_id']\n",
    "                next_res = current_trace[task_id+1]['agent_id']\n",
    "\n",
    "                var_dic['case name'].append(trace_name)\n",
    "                var_dic['event location'].append(task_id)\n",
    "\n",
    "                if current_act == next_act:\n",
    "                    act_same_or_not = 1\n",
    "                else:\n",
    "                    act_same_or_not = 0\n",
    "                var_dic['activity same or not'].append(act_same_or_not)\n",
    "\n",
    "                if aver_time_per_act_pair[(current_act, next_act)] == 0:\n",
    "                    time_dur_per = 0\n",
    "                else:\n",
    "                    time_dur_per = (next_time - current_time).total_seconds() / aver_time_per_act_pair[(current_act, next_act)]\n",
    "                var_dic['time duration'].append(time_dur_per)\n",
    "\n",
    "                if next_res == current_res:\n",
    "                    var_dic['continuous event number'].append(conti_event_len)\n",
    "                    var_dic['handover or not'].append(0)\n",
    "                    conti_event_len += 1\n",
    "                else:\n",
    "                    var_dic['continuous event number'].append(conti_event_len)\n",
    "                    var_dic['handover or not'].append(1)\n",
    "                    handover_num += 1\n",
    "                    conti_event_len = 1\n",
    "\n",
    "    var_df = pd.DataFrame(var_dic)\n",
    "\n",
    "    return var_df, case_num, handover_num\n",
    "\n",
    "\n",
    "# train the logistic regression model 1\n",
    "def train_datasets_store_in_dic(dic_for_train):\n",
    "    var_df, train_case_num, train_hand_num = extract_df_for_train(dic_for_train)\n",
    "    # train the logistic regression model\n",
    "    X = var_df[['event location', 'activity same or not', 'time duration']]\n",
    "    y = var_df['handover or not']\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X[['event location', 'activity same or not', 'time duration']])\n",
    "    X_final = np.hstack([X_scaled])\n",
    "    # train logistic regression\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_final, y)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train logistic regression classifier 2 (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Extraction\n",
    "# logistic regression model 2 training (will be applied to label the resource per case)\n",
    "\n",
    "# extract the event sequence pairs\n",
    "def extract_same_and_diff_resource_pair_from_one_log(log_tuple):\n",
    "    df1 = log_tuple[0].rename(columns={\n",
    "            'case_id': 'case:concept:name',     # Replace with actual case column name\n",
    "            'activity_type': 'concept:name',  # Replace with actual activity column name\n",
    "            'timestamp': 'time:timestamp'  # Replace with actual timestamp column name\n",
    "        })\n",
    "    unclustered_log = log_converter.apply(df1)\n",
    "    # get the average time duration for activity pairs firstly\n",
    "    log_pattern_dic = {}\n",
    "    for trace_id in range(len(unclustered_log)):\n",
    "        trace_name = 'trace_' + str(trace_id)\n",
    "        current_trace = unclustered_log[trace_id]\n",
    "        case_pattern_dic = {}\n",
    "        pattern = [current_trace[0]['concept:name']]\n",
    "        for task_id in range(len(unclustered_log[trace_id])-1):\n",
    "            next_act = current_trace[task_id+1]['concept:name']\n",
    "            current_res = current_trace[task_id]['agent_id']\n",
    "            next_res = current_trace[task_id+1]['agent_id']\n",
    "            if current_res == next_res:\n",
    "                pattern.append(next_act)\n",
    "            else:\n",
    "                if current_res in case_pattern_dic.keys():\n",
    "                    case_pattern_dic[current_res].append(pattern)\n",
    "                else:\n",
    "                    case_pattern_dic[current_res] = [pattern]\n",
    "                pattern = [next_act]\n",
    "            # consider the end of the case\n",
    "            if (task_id + 1) == (len(unclustered_log[trace_id]) - 1):\n",
    "                if current_res in case_pattern_dic.keys():\n",
    "                    case_pattern_dic[current_res].append(pattern)\n",
    "                else:\n",
    "                    case_pattern_dic[current_res] = [pattern]\n",
    "\n",
    "        log_pattern_dic[trace_name] = case_pattern_dic\n",
    "\n",
    "    return log_pattern_dic\n",
    "\n",
    "\n",
    "# also preprocess the training data (give the same resource or different resources for each event sequence pair Y variable (0 or 1))\n",
    "def build_pairs_of_patterns(log_pattern_dic):\n",
    "    # extract all the label 1 firstly\n",
    "    pattern_1, pattern_2, label_l = [], [], []\n",
    "    for case, re_dic in log_pattern_dic.items():\n",
    "        if len(re_dic) > 1:\n",
    "            for res, pattern_l in re_dic.items():\n",
    "                if len(pattern_l) > 1:\n",
    "                    for i in range(len(pattern_l)-1):\n",
    "                        for j in range(i+1, len(pattern_l)):\n",
    "                            pattern_1.append(pattern_l[i])\n",
    "                            pattern_2.append(pattern_l[j])\n",
    "                            label_l.append(1)\n",
    "\n",
    "    # extract some of the label 2\n",
    "    for case_1, re_dic_1 in log_pattern_dic.items():\n",
    "        if len(re_dic_1) > 1:\n",
    "            for r1 in range(len(re_dic_1)-1):\n",
    "                for r2 in range(i+1, len(re_dic_1)):\n",
    "                    pattern_1.append(random.choice(list(re_dic_1.values())[r1]))\n",
    "                    pattern_2.append(random.choice(list(re_dic_1.values())[r2]))\n",
    "                    label_l.append(0)\n",
    "\n",
    "    pattern_dic = {'item1': pattern_1, 'item2': pattern_2, 'label': label_l}\n",
    "    pattern_df = pd.DataFrame(pattern_dic)\n",
    "\n",
    "    return pattern_df\n",
    "\n",
    "\n",
    "# preprocess the data (we define the variables)\n",
    "# 1. common activities (intersection, keeps minimum counts)\n",
    "# 2. length difference of the two patterns ((length_1 - length2))\n",
    "# 3. common activities dependencies (common dependency frequencies of two patterns / whole dependencies)\n",
    "def preprocess_pattern_df(pattern_df, exists_df = ''):\n",
    "    common_act_value, length_diff_value, common_dep_value, label_l = [], [], [], []\n",
    "    for i, item in pattern_df.iterrows():\n",
    "        pa_1 = item[0]\n",
    "        pa_2 = item[1]\n",
    "        label_l.append(item[2])\n",
    "\n",
    "        counter1 = Counter(pa_1)\n",
    "        counter2 = Counter(pa_2)\n",
    "        common_elements = counter1 & counter2  # Intersection: keeps minimum counts\n",
    "        num_common_elements = sum(common_elements.values())\n",
    "        common_act_value.append(num_common_elements*2 / (len(pa_1) + len(pa_2)))\n",
    "\n",
    "        length_diff_value.append(abs(len(pa_1)-len(pa_2)))\n",
    "\n",
    "        dep_1, dep_2 = [], []\n",
    "        num_common_dep = 0\n",
    "        if len(pa_1) == 1 or len(pa_2) == 1:\n",
    "            num_common_dep = 0\n",
    "        else:\n",
    "            for i1 in range(len(pa_1)-1):\n",
    "                dep_1.append((pa_1[i1], pa_1[i1+1]))\n",
    "            for i2 in range(len(pa_2)-1):\n",
    "                dep_2.append((pa_2[i2], pa_2[i2+1]))\n",
    "            dep_counter1 = Counter(dep_1)\n",
    "            dep_counter2 = Counter(dep_2)\n",
    "            common_dep = dep_counter1 & dep_counter2\n",
    "            num_common_dep = 2 * sum(common_dep.values()) / (len(dep_1) + len(dep_2))\n",
    "\n",
    "        common_dep_value.append(num_common_dep)\n",
    "\n",
    "    processed_dic = {'common_act_value': common_act_value, 'length_diff_value': length_diff_value, 'common_dep_value': common_dep_value, 'label_l': label_l}\n",
    "    processed_df = pd.DataFrame(processed_dic)\n",
    "\n",
    "    # concat dfs\n",
    "    if isinstance(exists_df, str) and exists_df == '':\n",
    "        output_df = processed_df\n",
    "    else:\n",
    "        output_df = pd.concat([exists_df, processed_df], ignore_index=True)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "# train the logistic regression model 2\n",
    "def apply_logistic_regression_get_model(processed_df):\n",
    "    X = processed_df[['common_act_value', 'length_diff_value', 'common_dep_value']]\n",
    "    y = processed_df['label_l']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X[['common_act_value', 'length_diff_value', 'common_dep_value']])\n",
    "    X_final = np.hstack([X_scaled])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    # train logistic regression\n",
    "    logistic_model = LogisticRegression()\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "\n",
    "    return logistic_model, X_test, y_test\n",
    "\n",
    "\n",
    "# for multiple test logs, they will be applied to the same logistic regression model 2\n",
    "def second_step_train_on_most_datasets(dic_for_train):\n",
    "    processed_df = ''\n",
    "    for dataset, inform_t in dic_for_train.items():\n",
    "        log_pattern_dic = extract_same_and_diff_resource_pair_from_one_log(inform_t)\n",
    "        pattern_df = build_pairs_of_patterns(log_pattern_dic)\n",
    "        processed_df = preprocess_pattern_df(pattern_df, processed_df)\n",
    "\n",
    "    logistic_model, X_test, y_test = apply_logistic_regression_get_model(processed_df)\n",
    "\n",
    "    return logistic_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Inference\n",
    "### Apply Step 1 --- handover detection (use Domain 1 and logistic regression classifier model 1) (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Inference\n",
    "# Step 1 --- handover detection\n",
    "\n",
    "# construct the test datasets for a single test log\n",
    "def extract_df_for_test(log_for_test):\n",
    "    # preprocess the test log\n",
    "    var_test_dic = {'case name':[], 'event location':[], 'activity same or not':[], 'time duration':[], 'handover or not':[]}\n",
    "    df1 = log_for_test[0].rename(columns={\n",
    "            'case_id': 'case:concept:name',     # Replace with actual case column name\n",
    "            'activity_type': 'concept:name',  # Replace with actual activity column name\n",
    "            'timestamp': 'time:timestamp'  # Replace with actual timestamp column name\n",
    "        })\n",
    "    unclustered_log = log_converter.apply(df1)\n",
    "    # get the average time duration for activity pairs firstly\n",
    "    time_per_act_pair = {}\n",
    "    aver_time_per_act_pair = {}\n",
    "    case_num = len(unclustered_log)\n",
    "    for trace_id in range(len(unclustered_log)):\n",
    "        trace_name = 'trace_' + str(trace_id)\n",
    "        current_trace = unclustered_log[trace_id]\n",
    "        for task_id in range(len(unclustered_log[trace_id])-1):\n",
    "            current_time = datetime.strptime(current_trace[task_id]['time:timestamp'][:19], log_for_test[1])\n",
    "            next_time = datetime.strptime(current_trace[task_id+1]['time:timestamp'][:19], log_for_test[1])\n",
    "            current_act = current_trace[task_id]['concept:name']\n",
    "            next_act = current_trace[task_id+1]['concept:name']\n",
    "            if (current_act, next_act) not in time_per_act_pair.keys():\n",
    "                time_per_act_pair[(current_act, next_act)] = [(next_time - current_time).total_seconds()]\n",
    "            else:\n",
    "                time_per_act_pair[(current_act, next_act)].append((next_time - current_time).total_seconds())\n",
    "\n",
    "    for pair, time_l in time_per_act_pair.items():\n",
    "        aver_time_per_act_pair[pair] = sum(time_l) / len(time_l)\n",
    "\n",
    "    for trace_id in range(len(unclustered_log)):\n",
    "        trace_name = 'trace_' + str(trace_id)\n",
    "        current_trace = unclustered_log[trace_id]\n",
    "        for task_id in range(len(unclustered_log[trace_id])-1):\n",
    "            current_time = datetime.strptime(current_trace[task_id]['time:timestamp'][:19], log_for_test[1])\n",
    "            next_time = datetime.strptime(current_trace[task_id+1]['time:timestamp'][:19], log_for_test[1])\n",
    "            current_act = current_trace[task_id]['concept:name']\n",
    "            next_act = current_trace[task_id+1]['concept:name']\n",
    "            current_res = current_trace[task_id]['agent_id']\n",
    "            next_res = current_trace[task_id+1]['agent_id']\n",
    "\n",
    "            var_test_dic['case name'].append(trace_name)\n",
    "            var_test_dic['event location'].append(task_id)\n",
    "\n",
    "            if current_act == next_act:\n",
    "                act_same_or_not = 1\n",
    "            else:\n",
    "                act_same_or_not = 0\n",
    "            var_test_dic['activity same or not'].append(act_same_or_not)\n",
    "\n",
    "            if aver_time_per_act_pair[(current_act, next_act)] == 0:\n",
    "                time_dur_per = 0\n",
    "            else:\n",
    "                time_dur_per = (next_time - current_time).total_seconds() / aver_time_per_act_pair[(current_act, next_act)]\n",
    "            var_test_dic['time duration'].append(time_dur_per)\n",
    "\n",
    "            if next_res == current_res:\n",
    "                var_test_dic['handover or not'].append(0)\n",
    "            else:\n",
    "                var_test_dic['handover or not'].append(1)\n",
    "\n",
    "    var_test_df = pd.DataFrame(var_test_dic)\n",
    "\n",
    "    return var_test_df, case_num\n",
    "\n",
    "\n",
    "# detect the handover for the test logs in the test log dictionary\n",
    "def handover_detection_for_test_dic(dic_for_test, domain_1, regression_model_1):\n",
    "    hand_dic = {}\n",
    "\n",
    "    log_l = list(dic_for_test.keys())\n",
    "    for i in range(len(dic_for_test)):\n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S')} {log_l[i]} handover detection Start\")\n",
    "        log_for_test = dic_for_test[log_l[i]]\n",
    "        distri_name, ci, num_zero_hand_case_percent = domain_1\n",
    "        max_zero_hand_percent = max(num_zero_hand_case_percent)\n",
    "\n",
    "        def get_distri_para_and_MLE(distri_name, data):\n",
    "            values_c_dic = {}\n",
    "            values, counts = np.unique(data, return_counts=True)\n",
    "            for i in range(len(values)):\n",
    "                values_c_dic[values[i]] = counts[i]\n",
    "\n",
    "            counts_l = []\n",
    "            values_l = []\n",
    "            for j in range(max(values)):\n",
    "                if j in values_c_dic.keys():\n",
    "                    values_l.append(j)\n",
    "                    counts_l.append(values_c_dic[j])\n",
    "                else:\n",
    "                    values_l.append(j)\n",
    "                    counts_l.append(0)\n",
    "\n",
    "            def get_square_distance(l1, l2):\n",
    "                dist = 0\n",
    "                for i in range(len(l1)):\n",
    "                    dist  = dist + (l1[i] - l2[i]) ** 2\n",
    "                return dist\n",
    "\n",
    "            counts_prob_l = counts_l / sum(counts_l)\n",
    "            if distri_name == 'Poisson':\n",
    "                lambda_hat = np.mean(data)\n",
    "                pred_l = stats.poisson.pmf(values_l, lambda_hat)\n",
    "            if distri_name == 'Exponential':\n",
    "                lambda_hat = 1 / (np.mean(data) + 1)\n",
    "                pred_l = stats.expon.pdf(values_l, lambda_hat)\n",
    "            if distri_name == 'Geometric':\n",
    "                lambda_hat = 1 / np.mean(data) \n",
    "                pred_l = stats.geom.pmf(values_l, lambda_hat)\n",
    "            if distri_name == 'Constant':\n",
    "                lambda_hat = np.mean(data)\n",
    "                pred_l = [1 / len(values_l)] * len(values_l)\n",
    "            \n",
    "            distance = get_square_distance(counts_prob_l, pred_l)\n",
    "            return lambda_hat, distance\n",
    "\n",
    "        # get the test df\n",
    "        var_test_df, test_case_num = extract_df_for_test(log_for_test)\n",
    "\n",
    "        # scale the test data\n",
    "        X_prac = var_test_df[['event location', 'activity same or not', 'time duration']]\n",
    "        scaler = StandardScaler()\n",
    "        X_prac_scaled = scaler.fit_transform(X_prac[['event location', 'activity same or not', 'time duration']])\n",
    "        y_prac = var_test_df['handover or not']\n",
    "\n",
    "        # detect the handover for the test log and evaluate the precision\n",
    "        # set the threshold set, later we will choose the best one from this set\n",
    "        threshold_l = [round(0.01*i,2) for i in range(1, 100)]\n",
    "        thres_best = threshold_l[0]\n",
    "        lambda_best_diff = float(\"inf\")\n",
    "        lambda_in_ci_thres_dist_dic = {}\n",
    "        for threshold in threshold_l:\n",
    "            y_prac_pred_proba = regression_model_1.predict_proba(X_prac_scaled)[:, 1]\n",
    "            y_prac_pred_adjusted = (y_prac_pred_proba >= threshold).astype(int)\n",
    "            curr_case = var_test_df['case name'][0]\n",
    "            hand_num_l = []\n",
    "            hand_num_per_case = 0\n",
    "            for j, row in var_test_df.iterrows():\n",
    "                if row['case name'] == curr_case:\n",
    "                    if y_prac_pred_adjusted[j] == 1:\n",
    "                        hand_num_per_case += 1\n",
    "                else:\n",
    "                    hand_num_l.append(hand_num_per_case)\n",
    "                    curr_case = row['case name']\n",
    "                    if y_prac_pred_adjusted[j] == 1:\n",
    "                        hand_num_per_case = 1\n",
    "                    else:\n",
    "                        hand_num_per_case = 0\n",
    "            # get the last case handover num\n",
    "            hand_num_l.append(hand_num_per_case)\n",
    "            zero_percent = hand_num_l.count(0) / len(hand_num_l)\n",
    "            if zero_percent <= max_zero_hand_percent:\n",
    "                lambda_hat, distance = get_distri_para_and_MLE(distri_name, hand_num_l)\n",
    "                if lambda_hat > ci[1]:\n",
    "                    if (lambda_hat - ci[1]) < lambda_best_diff:\n",
    "                        thres_best = threshold\n",
    "                        lambda_best_diff = lambda_hat - ci[1]\n",
    "                elif lambda_hat < ci[0]:\n",
    "                    if (ci[0] - lambda_hat) < lambda_best_diff:\n",
    "                        thres_best = threshold\n",
    "                        lambda_best_diff = ci[0] - lambda_hat\n",
    "                elif lambda_hat >= ci[0] and lambda_hat <= ci[1]:\n",
    "                    lambda_in_ci_thres_dist_dic[threshold] = distance\n",
    "        \n",
    "        if lambda_in_ci_thres_dist_dic != {}:\n",
    "            thres_best = min(lambda_in_ci_thres_dist_dic, key=lambda k: (lambda_in_ci_thres_dist_dic[k], list(lambda_in_ci_thres_dist_dic).index(k)))\n",
    "\n",
    "        y_prac_pred_proba = regression_model_1.predict_proba(X_prac_scaled)[:, 1]\n",
    "        y_prac_pred_adjusted = (y_prac_pred_proba >= thres_best).astype(int)\n",
    "        prac_accuracy = accuracy_score(y_prac, y_prac_pred_adjusted)\n",
    "\n",
    "        hand_dic[log_l[i]] = (var_test_df, y_prac_pred_adjusted)\n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S')} {log_l[i]} handover detection Done\")\n",
    "\n",
    "    # return the handover detection results\n",
    "    return hand_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Step 2 --- resource labeling per case (use Domain 2 and logistic regression classifier model 2) (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Inference\n",
    "# Step 2 --- resource labeling per case\n",
    "\n",
    "# preprocess the output results from step 1 (make the handover detection results as an input to this Step 2)\n",
    "def construct_log_with_handover(df, var_test_df, y_prac_pred_adjusted):\n",
    "    var_test_df['predict handover'] = y_prac_pred_adjusted\n",
    "    df1 = df.rename(columns={\n",
    "            'case_id': 'case:concept:name',     # Replace with actual case column name\n",
    "            'activity_type': 'concept:name',  # Replace with actual activity column name\n",
    "            'timestamp': 'time:timestamp'  # Replace with actual timestamp column name\n",
    "        })\n",
    "    unclustered_log = log_converter.apply(df1)\n",
    "    log_pattern_dic = {}\n",
    "    for trace_id in range(len(unclustered_log)):\n",
    "        trace_name = 'trace_' + str(trace_id)\n",
    "        current_trace = unclustered_log[trace_id]\n",
    "        case_pattern_l = []\n",
    "        handover_for_the_case = var_test_df[(var_test_df['case name'] == trace_name) & (var_test_df['predict handover'] == 1)]\n",
    "        position_l = handover_for_the_case['event location'].tolist()\n",
    "        pattern_l = []\n",
    "        for task_id in range(len(unclustered_log[trace_id])):\n",
    "            if task_id not in position_l:\n",
    "                pattern_l.append(current_trace[task_id]['concept:name'])\n",
    "            else:\n",
    "                pattern_l.append(current_trace[task_id]['concept:name'])\n",
    "                case_pattern_l.append(pattern_l)\n",
    "                pattern_l = []\n",
    "\n",
    "            if task_id == (len(unclustered_log[trace_id]) - 1):\n",
    "                case_pattern_l.append(pattern_l)\n",
    "                pattern_l = []\n",
    "\n",
    "        log_pattern_dic[trace_name] = case_pattern_l\n",
    "    \n",
    "    return log_pattern_dic\n",
    "\n",
    "\n",
    "# preprocess the test dataset\n",
    "def preprocess_pattern_df_test(pattern_df):\n",
    "    common_act_value, length_diff_value, common_dep_value = [], [], []\n",
    "    for i, item in pattern_df.iterrows():\n",
    "        pa_1 = item[0]\n",
    "        pa_2 = item[1]\n",
    "\n",
    "        counter1 = Counter(pa_1)\n",
    "        counter2 = Counter(pa_2)\n",
    "        common_elements = counter1 & counter2  # Intersection: keeps minimum counts\n",
    "        num_common_elements = sum(common_elements.values())\n",
    "        common_act_value.append(num_common_elements*2 / (len(pa_1) + len(pa_2)))\n",
    "\n",
    "        length_diff_value.append(abs(len(pa_1)-len(pa_2)))\n",
    "\n",
    "        dep_1, dep_2 = [], []\n",
    "        num_common_dep = 0\n",
    "        if len(pa_1) == 1 or len(pa_2) == 1:\n",
    "            num_common_dep = 0\n",
    "        else:\n",
    "            for i1 in range(len(pa_1)-1):\n",
    "                dep_1.append((pa_1[i1], pa_1[i1+1]))\n",
    "            for i2 in range(len(pa_2)-1):\n",
    "                dep_2.append((pa_2[i2], pa_2[i2+1]))\n",
    "            dep_counter1 = Counter(dep_1)\n",
    "            dep_counter2 = Counter(dep_2)\n",
    "            common_dep = dep_counter1 & dep_counter2\n",
    "            num_common_dep = 2 * sum(common_dep.values()) / (len(dep_1) + len(dep_2))\n",
    "\n",
    "        common_dep_value.append(num_common_dep)\n",
    "\n",
    "    processed_dic = {'common_act_value': common_act_value, 'length_diff_value': length_diff_value, 'common_dep_value': common_dep_value}\n",
    "    processed_df = pd.DataFrame(processed_dic)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "# get the prediction results (in the dataframe format)\n",
    "def apply_logistic_detect_resource_for_case(log_pattern_dic, logistic_model, domain_2, file_path):\n",
    "    # set the threshold set, later we will choose the best one from this set\n",
    "    threshold_l = [round(0.05*i,2) for i in range(1, 20)]\n",
    "    thres_best = threshold_l[0]\n",
    "    lambda_best_diff = float(\"inf\")\n",
    "    lambda_in_ci_thres_dist_dic = {}\n",
    "    best_whole_case_list, best_pattern_list, best_res_list = [], [], []\n",
    "    distri_name, ci, zero_res_percent = domain_2\n",
    "    for threshold in threshold_l:\n",
    "        whole_case_list, pattern_list, res_list = [], [], []\n",
    "        for case, pattern_l in log_pattern_dic.items():\n",
    "            if len(pattern_l) <= 1:\n",
    "                whole_case_list.append(case)\n",
    "                pattern_list.append(pattern_l[0])\n",
    "                res_list.append('R1')\n",
    "            else:\n",
    "                pattern_1, pattern_2, resource_l, i_j_l = [], [], [], []\n",
    "                possible_res = {} # will be {1:[pattern_1, patterm_2], 2:[pattern_3], ...}\n",
    "                current_case_re_num = 0\n",
    "                for i in range(len(pattern_l)-1):\n",
    "                    for j in range(i+1, len(pattern_l)):\n",
    "                        pattern_1.append(pattern_l[i])\n",
    "                        pattern_2.append(pattern_l[j])\n",
    "                        i_j_l.append((i, j))\n",
    "\n",
    "                pattern_dic = {'item1': pattern_1, 'item2': pattern_2}\n",
    "                pattern_df = pd.DataFrame(pattern_dic)\n",
    "\n",
    "                processed_df = preprocess_pattern_df_test(pattern_df)\n",
    "\n",
    "                X_test = processed_df[['common_act_value', 'length_diff_value', 'common_dep_value']]\n",
    "\n",
    "                # predict the relationships using the logistic regression model\n",
    "                i_j_pa_dic = {}\n",
    "                y_pred_proba = logistic_model.predict_proba(X_test)[:, 1]\n",
    "                for i1 in range(len(i_j_l)):\n",
    "                    i_j_pa_dic[i_j_l[i1]] = y_pred_proba[i1]\n",
    "                \n",
    "                whole_case_list.append(case)\n",
    "                pattern_list.append(pattern_l[0])\n",
    "                current_case_re_num += 1\n",
    "                res_list.append('R1')\n",
    "                possible_res[1] = [0]\n",
    "\n",
    "                whole_case_list.append(case)\n",
    "                pattern_list.append(pattern_l[1])\n",
    "                current_case_re_num += 1\n",
    "                res_list.append('R2')\n",
    "                possible_res[2] = [1]\n",
    "\n",
    "                for i2 in range(2, len(pattern_l)):\n",
    "                    res_before = next((k for k, v in possible_res.items() if (i2 - 1) in v), None)\n",
    "                    pat_cannot_calculate = possible_res[res_before]\n",
    "                    max_prob = threshold\n",
    "                    max_res = -1\n",
    "                    for (i3, j3), prob_v in i_j_pa_dic.items():\n",
    "                        if (j3 == i2) and (i3 not in pat_cannot_calculate):\n",
    "                            if prob_v > max_prob:\n",
    "                                max_res = next((k for k, v in possible_res.items() if i3 in v), None)\n",
    "                                max_prob = prob_v\n",
    "\n",
    "                    if max_res != -1:\n",
    "                        res_str = 'R' + str(max_res)\n",
    "                        whole_case_list.append(case)\n",
    "                        pattern_list.append(pattern_l[i2])\n",
    "                        res_list.append(res_str)\n",
    "                        possible_res[max_res].append(i2)\n",
    "                    else:\n",
    "                        current_case_re_num += 1\n",
    "                        res_str = 'R' + str(current_case_re_num)\n",
    "                        whole_case_list.append(case)\n",
    "                        pattern_list.append(pattern_l[i2])\n",
    "                        res_list.append(res_str)\n",
    "                        possible_res[current_case_re_num] = [i2]\n",
    "\n",
    "        if threshold == threshold_l[0]:\n",
    "            best_whole_case_list = whole_case_list\n",
    "            best_pattern_list = pattern_list\n",
    "            best_res_list = res_list\n",
    "\n",
    "        def get_distri_para_and_MLE(distri_name, data):\n",
    "            values_c_dic = {}\n",
    "            values, counts = np.unique(data, return_counts=True)\n",
    "            for i in range(len(values)):\n",
    "                values_c_dic[values[i]] = counts[i]\n",
    "\n",
    "            counts_l = []\n",
    "            values_l = []\n",
    "            for j in range(max(values)):\n",
    "                if j in values_c_dic.keys():\n",
    "                    values_l.append(j)\n",
    "                    counts_l.append(values_c_dic[j])\n",
    "                else:\n",
    "                    values_l.append(j)\n",
    "                    counts_l.append(0)\n",
    "\n",
    "            def get_square_distance(l1, l2):\n",
    "                dist = 0\n",
    "                for i in range(len(l1)):\n",
    "                    dist  = dist + (l1[i] - l2[i]) ** 2\n",
    "                return dist\n",
    "\n",
    "            counts_prob_l = counts_l / sum(counts_l)\n",
    "            if distri_name == 'Poisson':\n",
    "                lambda_hat = np.mean(data)\n",
    "                pred_l = stats.poisson.pmf(values_l, lambda_hat)\n",
    "            if distri_name == 'Exponential':\n",
    "                lambda_hat = 1 / (np.mean(data) + 1)\n",
    "                pred_l = stats.expon.pdf(values_l, lambda_hat)\n",
    "            if distri_name == 'Geometric':\n",
    "                lambda_hat = 1 / np.mean(data) \n",
    "                pred_l = stats.geom.pmf(values_l, lambda_hat)\n",
    "            if distri_name == 'Constant':\n",
    "                lambda_hat = np.mean(data)\n",
    "                pred_l = [1 / len(values_l)] * len(values_l)\n",
    "            \n",
    "            distance = get_square_distance(counts_prob_l, pred_l)\n",
    "            \n",
    "            return lambda_hat, distance\n",
    "\n",
    "        res_num_l = []\n",
    "        res_single_case_l = [res_list[0]]\n",
    "        for j in range(len(whole_case_list)-1):\n",
    "            if whole_case_list[j+1] == whole_case_list[j]:\n",
    "                if res_list[j+1] not in res_single_case_l:\n",
    "                    res_single_case_l.append(res_list[j+1])\n",
    "            else:\n",
    "                res_num_l.append(len(res_single_case_l)-1) # minus the res_num by 1, for better fits\n",
    "                res_single_case_l = [res_list[j+1]]\n",
    "        # append the final value\n",
    "        res_num_l.append(len(res_single_case_l))\n",
    "        max_zero_res_percent = max(zero_res_percent)\n",
    "        zero_percent = res_num_l.count(0) / len(res_num_l)\n",
    "        if zero_percent <= max_zero_res_percent:\n",
    "            lambda_hat, distance = get_distri_para_and_MLE(distri_name, res_num_l)\n",
    "            if lambda_hat > ci[1]:\n",
    "                if (lambda_hat - ci[1]) < lambda_best_diff:\n",
    "                    thres_best = threshold\n",
    "                    lambda_best_diff = lambda_hat - ci[1]\n",
    "                    best_whole_case_list = whole_case_list\n",
    "                    best_pattern_list = pattern_list\n",
    "                    best_res_list = res_list\n",
    "            elif lambda_hat < ci[0]:\n",
    "                if (ci[0] - lambda_hat) < lambda_best_diff:\n",
    "                    thres_best = threshold\n",
    "                    lambda_best_diff = ci[0] - lambda_hat\n",
    "                    best_whole_case_list = whole_case_list\n",
    "                    best_pattern_list = pattern_list\n",
    "                    best_res_list = res_list\n",
    "            elif lambda_hat >= ci[0] and lambda_hat <= ci[1]:\n",
    "                lambda_in_ci_thres_dist_dic[threshold] = (distance, whole_case_list, pattern_list, res_list)\n",
    "\n",
    "    if lambda_in_ci_thres_dist_dic != {}:\n",
    "        thres_best = min(lambda_in_ci_thres_dist_dic, key=lambda k: lambda_in_ci_thres_dist_dic[k][0])\n",
    "        best_whole_case_list, best_pattern_list, best_res_list = lambda_in_ci_thres_dist_dic[thres_best][1:4]\n",
    "\n",
    "    best_pattern_with_resource_dic = {'whole case list': best_whole_case_list, 'pattern list': best_pattern_list, 'res list': best_res_list}\n",
    "    pattern_with_resource_df = pd.DataFrame(best_pattern_with_resource_dic)\n",
    "    pattern_with_resource_df.to_csv(file_path)\n",
    "\n",
    "    return pattern_with_resource_df\n",
    "\n",
    "\n",
    "# finalize the Step 2 and store the output of this step into a file path\n",
    "def create_event_df_after_step_two(df_dic_with_time_train, df_dic_with_time_test, test_df_with_y_pred_dic, domain_2, lrc_model_2, f_path):\n",
    "    eve_log_l = list(df_dic_with_time_test.keys())\n",
    "    for eve_log in eve_log_l:\n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S')} {eve_log} resource labeling per case Start\")\n",
    "        (var_test_df, y_prac_pred_adjusted) = test_df_with_y_pred_dic[eve_log]\n",
    "        logistic_model = lrc_model_2\n",
    "        log_pattern_dic = construct_log_with_handover(df_dic_with_time_test[eve_log][0], var_test_df, y_prac_pred_adjusted)\n",
    "        file_path = f_path + '/' + str(eve_log) + '_middle_output.csv'\n",
    "        apply_logistic_detect_resource_for_case(log_pattern_dic, logistic_model, domain_2, file_path)\n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S')} {eve_log} Step 2 middle outputs stored successfully\")\n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S')} {eve_log} resource labeling per case Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Step 3 --- resource assignment for the log (use Domain 3 and Domain 4) (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Inference\n",
    "# Step 3 --- resource assignment for the log\n",
    "\n",
    "# collect the domain 3 and domain 4 information\n",
    "def get_aver_res_rank_act_and_case_from_train_set(dic_for_train):\n",
    "    res_rank_act_perc_dic = extract_aver_resource_rank_activity_freq_without_plot(dic_for_train)\n",
    "    res_rank_case_perc_dic = get_resource_involve_case_num_without_plot(dic_for_train)\n",
    "\n",
    "    return res_rank_act_perc_dic, res_rank_case_perc_dic\n",
    "\n",
    "\n",
    "# case_ids = [0, 0, 1, 1, 2, 2]  # 6 events across 3 cases\n",
    "# event_groups = [[0, 1], [2, 3], [4, 5]]  # Each group spans a case\n",
    "# target_event_counts = [2, 2, 2]  # For 3 resources\n",
    "# target_case_counts = [2, 2, 2]\n",
    "# the main of the simulated annealing step\n",
    "def simulated_annealing_with_groups(\n",
    "    case_ids,\n",
    "    target_event_counts,\n",
    "    target_case_counts,\n",
    "    num_resources,\n",
    "    event_groups,  # List of groups, e.g., [[0,1], [2,3]] where events in the same group must share a resource\n",
    "    case_event_normalize, # use it to normalize the result (normalize = total case num / total event num)\n",
    "    initial_temp=10000,\n",
    "    cooling_rate=0.95,\n",
    "    max_iter=1000,\n",
    "):\n",
    "    num_events = len(case_ids)\n",
    "    current_assignments = [0] * num_events\n",
    "\n",
    "    # Initialize grouped events with the same resource\n",
    "    for group in event_groups:\n",
    "        r = random.randint(0, num_resources - 1)\n",
    "        for event_idx in group:\n",
    "            current_assignments[event_idx] = r\n",
    "\n",
    "    # Initialize non-grouped events randomly\n",
    "    all_grouped_events = set(event_idx for group in event_groups for event_idx in group)\n",
    "    non_group_events = [idx for idx in range(num_events) if idx not in all_grouped_events]\n",
    "    for idx in non_group_events:\n",
    "        current_assignments[idx] = random.randint(0, num_resources - 1)\n",
    "\n",
    "    # Track event counts and case assignments efficiently\n",
    "    event_counts = np.zeros(num_resources, dtype=int)\n",
    "    case_event_counts = [defaultdict(int) for _ in range(num_resources)]\n",
    "    case_sets = [set() for _ in range(num_resources)]\n",
    "\n",
    "    for i, r in enumerate(current_assignments):\n",
    "        event_counts[r] += 1\n",
    "        case_id = case_ids[i]\n",
    "        case_event_counts[r][case_id] += 1\n",
    "        if case_event_counts[r][case_id] == 1:\n",
    "            case_sets[r].add(case_id)\n",
    "\n",
    "    # Compute initial cost\n",
    "    current_cost = 0\n",
    "    for r in range(num_resources):\n",
    "        current_cost += (event_counts[r] - target_event_counts[r]) ** 2\n",
    "        current_cost += (len(case_sets[r]) - target_case_counts[r]) ** 2\n",
    "\n",
    "    best_assignments = current_assignments.copy()\n",
    "    best_cost = current_cost\n",
    "    temperature = initial_temp\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        # Decide whether to modify a group or an ungrouped event\n",
    "        if non_group_events and random.random() < 0.5:\n",
    "            # Move: Change an ungrouped event\n",
    "            event_idx = random.choice(non_group_events)\n",
    "            old_r = current_assignments[event_idx]\n",
    "            new_r = random.choice([r for r in range(num_resources) if r != old_r])\n",
    "            case_id = case_ids[event_idx]\n",
    "\n",
    "            # Calculate delta cost\n",
    "            delta_event = ((event_counts[old_r] - 1 - target_event_counts[old_r]) ** 2 - (event_counts[old_r] - target_event_counts[old_r]) ** 2) + \\\n",
    "                          ((event_counts[new_r] + 1 - target_event_counts[new_r]) ** 2 - (event_counts[new_r] - target_event_counts[new_r]) ** 2)\n",
    "\n",
    "            # Case count delta for old_r\n",
    "            if case_event_counts[old_r][case_id] == 1:\n",
    "                delta_case_old = (len(case_sets[old_r]) - 1 - target_case_counts[old_r]) ** 2 - (len(case_sets[old_r]) - target_case_counts[old_r]) ** 2\n",
    "            else:\n",
    "                delta_case_old = 0\n",
    "\n",
    "            # Case count delta for new_r\n",
    "            if case_event_counts[new_r][case_id] == 0:\n",
    "                delta_case_new = (len(case_sets[new_r]) + 1 - target_case_counts[new_r]) ** 2 - (len(case_sets[new_r]) - target_case_counts[new_r]) ** 2\n",
    "            else:\n",
    "                delta_case_new = 0\n",
    "            \n",
    "            # modify it with the normalized one\n",
    "            total_delta = delta_event * (case_event_normalize ** 2) + (delta_case_old + delta_case_new)\n",
    "\n",
    "            # Accept/reject\n",
    "            if total_delta < 0 or random.random() < math.exp(-total_delta / temperature):\n",
    "                # Update assignments and trackers\n",
    "                current_assignments[event_idx] = new_r\n",
    "                event_counts[old_r] -= 1\n",
    "                event_counts[new_r] += 1\n",
    "\n",
    "                case_event_counts[old_r][case_id] -= 1\n",
    "                if case_event_counts[old_r][case_id] == 0:\n",
    "                    case_sets[old_r].remove(case_id)\n",
    "                case_event_counts[new_r][case_id] += 1\n",
    "                if case_event_counts[new_r][case_id] == 1:\n",
    "                    case_sets[new_r].add(case_id)\n",
    "\n",
    "                current_cost += total_delta\n",
    "                if current_cost < best_cost:\n",
    "                    best_assignments = current_assignments.copy()\n",
    "                    best_cost = current_cost\n",
    "\n",
    "        else:\n",
    "            # Move: Change a group's resource\n",
    "            group = random.choice(event_groups)\n",
    "            old_r = current_assignments[group[0]]\n",
    "            new_r = random.choice([r for r in range(num_resources) if r != old_r])\n",
    "            case_id = case_ids[group[0]]\n",
    "            group_size = len(group)\n",
    "\n",
    "            # Calculate delta event cost\n",
    "            delta_event = ((event_counts[old_r] - group_size - target_event_counts[old_r]) ** 2 - (event_counts[old_r] - target_event_counts[old_r]) ** 2) + \\\n",
    "                          ((event_counts[new_r] + group_size - target_event_counts[new_r]) ** 2 - (event_counts[new_r] - target_event_counts[new_r]) ** 2)\n",
    "\n",
    "            # Case count delta for old_r\n",
    "            if case_event_counts[old_r][case_id] == group_size:\n",
    "                delta_case_old = (len(case_sets[old_r]) - 1 - target_case_counts[old_r]) ** 2 - (len(case_sets[old_r]) - target_case_counts[old_r]) ** 2\n",
    "            else:\n",
    "                delta_case_old = 0\n",
    "\n",
    "            # Case count delta for new_r\n",
    "            if case_event_counts[new_r][case_id] == 0:\n",
    "                delta_case_new = (len(case_sets[new_r]) + 1 - target_case_counts[new_r]) ** 2 - (len(case_sets[new_r]) - target_case_counts[new_r]) ** 2\n",
    "            else:\n",
    "                delta_case_new = 0\n",
    "\n",
    "            total_delta = delta_event + delta_case_old + delta_case_new\n",
    "\n",
    "            # Accept/reject\n",
    "            if total_delta < 0 or random.random() < math.exp(-total_delta / temperature):\n",
    "                # Update assignments and trackers for the entire group\n",
    "                for event_idx in group:\n",
    "                    current_assignments[event_idx] = new_r\n",
    "\n",
    "                event_counts[old_r] -= group_size\n",
    "                event_counts[new_r] += group_size\n",
    "\n",
    "                case_event_counts[old_r][case_id] -= group_size\n",
    "                if case_event_counts[old_r][case_id] == 0:\n",
    "                    case_sets[old_r].remove(case_id)\n",
    "\n",
    "                case_event_counts[new_r][case_id] += group_size\n",
    "                if case_event_counts[new_r][case_id] == group_size:\n",
    "                    case_sets[new_r].add(case_id)\n",
    "\n",
    "                current_cost += total_delta\n",
    "                if current_cost < best_cost:\n",
    "                    best_assignments = current_assignments.copy()\n",
    "                    best_cost = current_cost\n",
    "\n",
    "        temperature *= cooling_rate\n",
    "\n",
    "    return best_assignments, best_cost\n",
    "\n",
    "\n",
    "# case_ids = [0, 0, 1, 1, 2, 2]  # 6 events across 3 cases\n",
    "# event_groups = [[0, 1], [2, 3], [4, 5]]  # Each group spans a case\n",
    "# target_event_counts = [2, 2, 2]  # For 3 resources\n",
    "# target_case_counts = [2, 2, 2]\n",
    "# preprocess the outputs from Step 2 (the stored file), to make the data as the input for Step 3\n",
    "def assign_res_for_a_single_log(log_name, res_rank_act_perc_dic, res_rank_case_perc_dic, file_p):\n",
    "    # get the csv file path after the step 2\n",
    "    file_path = f'{file_p}/{log_name}_middle_output.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['pattern list'] = data['pattern list'].apply(ast.literal_eval)\n",
    "    case_ids, event_groups = [], []\n",
    "    case_id = -1\n",
    "    case_name = 'trace'\n",
    "    event_id = 0\n",
    "    for i, row in data.iterrows():\n",
    "        if row['whole case list'] != case_name:\n",
    "            if i != 0:\n",
    "                for event_group in res_event_per_case_dic.values():\n",
    "                    event_groups.append(event_group)\n",
    "            res_event_per_case_dic = {}\n",
    "            case_id += 1\n",
    "            event_group = []\n",
    "            for _ in row['pattern list']:\n",
    "                case_ids.append(case_id)\n",
    "                event_group.append(event_id)\n",
    "                event_id += 1\n",
    "            res_event_per_case_dic[row['res list']] = event_group\n",
    "            case_name = row['whole case list']\n",
    "        else:\n",
    "            if row['res list'] not in res_event_per_case_dic.keys():\n",
    "                event_group = []\n",
    "                for _ in row['pattern list']:\n",
    "                    case_ids.append(case_id)\n",
    "                    event_group.append(event_id)\n",
    "                    event_id += 1\n",
    "                res_event_per_case_dic[row['res list']] = event_group\n",
    "            else:\n",
    "                for _ in row['pattern list']:\n",
    "                    case_ids.append(case_id)\n",
    "                    res_event_per_case_dic[row['res list']].append(event_id)\n",
    "                    event_id += 1\n",
    "\n",
    "    # deal with the act and case dics\n",
    "    event_num = event_id\n",
    "    case_num = case_id + 1\n",
    "    case_event_normalize = case_num / event_num\n",
    "    target_event_l = [int(event_num * per) for per in res_rank_act_perc_dic.values()]\n",
    "    target_case_l = [int(case_num * per) for per in res_rank_case_perc_dic.values()]\n",
    "\n",
    "    return case_ids, event_groups, target_event_l, target_case_l, case_event_normalize\n",
    "\n",
    "\n",
    "# construct final output event logs using the assigned resource attributes\n",
    "def add_resource_attr_to_csv(res_assignments, data, new_file_path, df_name, log_name, whether_TSI=False):\n",
    "    data_1 = data\n",
    "    if \"agent_id\" in data.columns:\n",
    "        data_1 = data.drop(columns=[\"agent_id\"], inplace=False)\n",
    "    if \"agent_activity_type\" in data.columns:\n",
    "        data_2 = data_1.drop(columns=[\"agent_activity_type\"], inplace=False)\n",
    "    else:\n",
    "        data_2 = data_1\n",
    "    # add the attributes with assigned resources\n",
    "    res_l = []\n",
    "    res_act_type_l = []\n",
    "    for r in res_assignments:\n",
    "        res = 'A' + str(r+1)\n",
    "        res_l.append(res)\n",
    "    for i, row in data_1.iterrows():\n",
    "        act = row['activity_type']\n",
    "        res_act_type = res_l[i] + \"|\" + act\n",
    "        res_act_type_l.append(res_act_type)\n",
    "\n",
    "    data_2['agent_id'] = res_l\n",
    "    data_2['agent_activity_type'] = res_act_type_l\n",
    "\n",
    "    data_2.to_csv(new_file_path, index=False)\n",
    "\n",
    "    if whether_TSI == True:\n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S')} {df_name} Step 3 final outputs stored successfully\")\n",
    "    else:\n",
    "        print(f\"{log_name} stored successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the three-step resource inference pipeline (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation methods\n",
    "\n",
    "# the evaluation cell (the mapping approach)\n",
    "# map one predicted resource to the existed resource, then get the maximum precision\n",
    "def get_theory_one_information_from_log_1(mas_log):\n",
    "    # resource_action_dic = {R1:{T1:2,T2:4,...}, R2:[T2:1,T3:3,...]}\n",
    "    # action_resource_dic = {T1:[R1:1,R2:3,...], T2:[R3:4,R4:1,...]}\n",
    "    # case_action = {C1:[T1,T2,T3, ...], C2:[T1,T2,...], ...}\n",
    "    resource_action_dic = {}\n",
    "    action_resource_dic = {}\n",
    "    case_action_dic = {}\n",
    "    case_name = -1\n",
    "    # different resources set\n",
    "    diff_resource_list = []\n",
    "    # different actions set\n",
    "    diff_task_list = []\n",
    "    # collect all the resources\n",
    "    whole_resource_l = []\n",
    "    for trace_id in range(len(mas_log)):\n",
    "        case_name = 'Case_' + str(trace_id)\n",
    "        current_trace = mas_log[trace_id]\n",
    "        case_action_l = []\n",
    "        for task_id in range(len(mas_log[trace_id])):\n",
    "            current_resource = current_trace[task_id]['agent_id']\n",
    "            whole_resource_l.append(current_resource)\n",
    "            current_task_name = current_trace[task_id]['concept:name']\n",
    "            case_action_l.append(current_task_name)\n",
    "            if current_resource not in diff_resource_list:\n",
    "                diff_resource_list.append(current_resource)\n",
    "            if current_task_name not in diff_task_list:\n",
    "                diff_task_list.append(current_task_name)\n",
    "            if current_resource not in resource_action_dic.keys():\n",
    "                act_dic = {}\n",
    "                act_dic[current_task_name] = 1\n",
    "                resource_action_dic[current_resource] = act_dic\n",
    "            else:\n",
    "                if current_task_name not in resource_action_dic[current_resource].keys():\n",
    "                    resource_action_dic[current_resource][current_task_name] = 1\n",
    "                else:\n",
    "                    resource_action_dic[current_resource][current_task_name] += 1\n",
    "\n",
    "            if current_task_name not in action_resource_dic.keys():\n",
    "                res_dic = {}\n",
    "                res_dic[current_resource] = 1\n",
    "                action_resource_dic[current_task_name] = res_dic\n",
    "            else:\n",
    "                if current_resource not in action_resource_dic[current_task_name].keys():\n",
    "                    action_resource_dic[current_task_name][current_resource] = 1\n",
    "                else:\n",
    "                    action_resource_dic[current_task_name][current_resource] += 1\n",
    "\n",
    "        case_action_dic[case_name] = case_action_l\n",
    "\n",
    "    return resource_action_dic, action_resource_dic, case_action_dic, diff_resource_list, diff_task_list, whole_resource_l\n",
    "\n",
    "\n",
    "def evaluation_approach_one(diff_res_list, pred_res_l, whole_res_l, pred_whole_l):\n",
    "    # sort the diff_res_list, trigger most events to trigger least events\n",
    "    gd_freq_res_dic = {}\n",
    "    pred_freq_res_dic = {}\n",
    "    for gd_res in diff_res_list:\n",
    "        gd_freq_res_dic[gd_res] = 0\n",
    "    for res in whole_res_l:\n",
    "        gd_freq_res_dic[res] += 1\n",
    "\n",
    "    for pred_res in pred_res_l:\n",
    "        pred_freq_res_dic[pred_res] = 0\n",
    "    for res1 in pred_whole_l:\n",
    "        pred_freq_res_dic[res1] += 1\n",
    "\n",
    "    pred_res_same_num_dic = {} # {'r1':[1,3,5], 'r2':[2,4,6], ...}\n",
    "    for pred_res in pred_res_l:\n",
    "        pred_res_same_num_dic[pred_res] = {}\n",
    "        for diff_res in diff_res_list:\n",
    "            pred_res_same_num_dic[pred_res][diff_res] = 0\n",
    "\n",
    "    for i in range(len(whole_res_l)):\n",
    "        pred_res_same_num_dic[pred_whole_l[i]][whole_res_l[i]] += 1\n",
    "\n",
    "    # get the plain dictionary and sort it\n",
    "    plain_dic = {}\n",
    "    for pred1, item1 in pred_res_same_num_dic.items():\n",
    "        for gd1, num in item1.items():\n",
    "            plain_dic[(pred1, gd1)] = num\n",
    "\n",
    "    sorted_plain_dic = dict(sorted(plain_dic.items(), key=lambda item: item[1], reverse=True))\n",
    "    record_pred_res_l, record_gd_res_l = [], []\n",
    "    pred_gd_res_map = {}\n",
    "    corr_num_min = 0\n",
    "    for (pred_res, gd_res) in sorted_plain_dic.keys():\n",
    "        if pred_res not in record_pred_res_l and gd_res not in record_gd_res_l:\n",
    "            corr_num_min += sorted_plain_dic[(pred_res, gd_res)]\n",
    "            pred_gd_res_map[(pred_res, gd_res)] = sorted_plain_dic[(pred_res, gd_res)]\n",
    "            record_pred_res_l.append(pred_res)\n",
    "            record_gd_res_l.append(gd_res)\n",
    "        if len(record_gd_res_l) == len(diff_res_list):\n",
    "            break\n",
    "            \n",
    "    print(f\"{' ' * 8} Resource prediction accuracy: {round(corr_num_min / len(whole_res_l), 4)}\")\n",
    "\n",
    "\n",
    "def evaluation_approach_two(case_ids, whole_resource_l, best_assignments):\n",
    "    total_num = 0\n",
    "    pred_acc_num = 0\n",
    "    for i in range(len(case_ids)-1):\n",
    "        f_eve_c = case_ids[i]\n",
    "        s_eve_c = case_ids[i+1]\n",
    "        if f_eve_c == s_eve_c:\n",
    "            total_num += 1\n",
    "            if (whole_resource_l[i] == whole_resource_l[i+1] and best_assignments[i] == best_assignments[i+1]) or (whole_resource_l[i] != whole_resource_l[i+1] and best_assignments[i] != best_assignments[i+1]):\n",
    "                pred_acc_num += 1\n",
    "\n",
    "    print(f\"{' ' * 8} Event pair resource handover accuracy: {round(pred_acc_num / total_num, 4)}\")\n",
    "\n",
    "\n",
    "def evaluation_pipeline(df_dic_with_time_test, dataset_name, case_ids, best_assignments):\n",
    "    for df_name, df in df_dic_with_time_test.items():\n",
    "        if df_name == dataset_name:\n",
    "            df1 = df[0].rename(columns={\n",
    "                'case_id': 'case:concept:name',     # Replace with actual case column name\n",
    "                'activity_type': 'concept:name',  # Replace with actual activity column name\n",
    "                'timestamp': 'time:timestamp'  # Replace with actual timestamp column name\n",
    "            })\n",
    "            unclustered_log = log_converter.apply(df1)\n",
    "\n",
    "            pred_res_l = []\n",
    "            for i in best_assignments:\n",
    "                if i not in pred_res_l:\n",
    "                    pred_res_l.append(i)\n",
    "\n",
    "            resource_action_dic, action_resource_dic, case_action_dic, diff_resource_list, diff_task_list, whole_resource_l = get_theory_one_information_from_log_1(unclustered_log)\n",
    "            evaluation_approach_one(diff_resource_list, pred_res_l, whole_resource_l, best_assignments)\n",
    "            evaluation_approach_two(case_ids, whole_resource_l, best_assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the three-step resource inference pipeline and the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:24:51 Three-step resource inference technique pipeline Start\n",
      "\n",
      "22:24:51 Phase 1: Extraction Start\n",
      "\n",
      "22:24:57 Domain 1 extracted successfully\n",
      "22:25:02 Domain 2 extracted successfully\n",
      "22:25:13 Domain 3 extracted successfully\n",
      "22:25:13 Domain 4 extracted successfully\n",
      "22:25:29 Logistic regression model 1 trained successfully\n",
      "22:25:58 Logistic regression model 2 trained successfully\n",
      "\n",
      "22:25:58 Phase 1: Extraction Done\n",
      "\n",
      "22:25:58 Phase 2: Inference Start\n",
      "\n",
      "22:25:58 Step 1 --- handover detection (for all test logs) Start\n",
      "22:25:58 bpic_2011 handover detection Start\n",
      "22:28:38 bpic_2011 handover detection Done\n",
      "22:28:38 Step 1 --- handover detection (for all test logs) Done\n",
      "\n",
      "22:28:38 Step 2 --- resource labeling per case (for all test logs) Start\n",
      "22:28:38 bpic_2011 resource labeling per case Start\n",
      "22:32:13 bpic_2011 Step 2 middle outputs stored successfully\n",
      "22:32:13 bpic_2011 resource labeling per case Done\n",
      "22:32:13 Step 2 --- resource labeling per case (for all test logs) Done\n",
      "\n",
      "22:32:13 Step 3 --- resource assignment for logs Start\n",
      "22:32:13 bpic_2011 resource assignment for the log Start\n",
      "22:32:26 bpic_2011 Step 3 final outputs stored successfully\n",
      "22:32:26 bpic_2011 resource assignment for the log Done\n",
      "22:32:26 bpic_2011 resource inferred log evaluation Start\n",
      "         Resource prediction accuracy: 0.2004\n",
      "         Event pair resource handover accuracy: 0.7263\n",
      "22:32:27 bpic_2011 resource inferred log evaluation Done\n",
      "22:32:27 Step 3 --- resource assignment for logs Done\n",
      "\n",
      "22:32:27 Phase 2: Inference Done\n",
      "\n",
      "22:32:27 Three-step resource inference technique pipeline Done\n"
     ]
    }
   ],
   "source": [
    "# run the three steps resource inference pipeline\n",
    "\n",
    "df_name_l = list(df_dic_with_time_train.keys())\n",
    "df_test_name_l = list(df_dic_with_time_test.keys())\n",
    "\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Three-step resource inference technique pipeline Start\")\n",
    "print()\n",
    "# Phase 1: Extraction\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Phase 1: Extraction Start\")\n",
    "print()\n",
    "# 1. extract Domain 1 from training logs\n",
    "domain_1 = get_case_hand_knowledge(df_dic_with_time_train)\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Domain 1 extracted successfully\")\n",
    "\n",
    "# 2. extract Domain 2 from training logs\n",
    "domain_2 = get_case_resource_knowledge(df_dic_with_time_train)\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Domain 2 extracted successfully\")\n",
    "\n",
    "# 3. extract Domain 3 and Domain 4 together\n",
    "domain_3, domain_4 = get_aver_res_rank_act_and_case_from_train_set(df_dic_with_time_train)\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Domain 3 extracted successfully\")\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Domain 4 extracted successfully\")\n",
    "\n",
    "# 4. train logistic regression classification model 1 (LRC1)\n",
    "lrc_model_1 = train_datasets_store_in_dic(df_dic_with_time_train)\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Logistic regression model 1 trained successfully\")\n",
    "\n",
    "# 5. train logistic regression classification model 2 (LRC2)\n",
    "lrc_model_2 = second_step_train_on_most_datasets(df_dic_with_time_train)\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Logistic regression model 2 trained successfully\")\n",
    "\n",
    "print()\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Phase 1: Extraction Done\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Phase 2: Inference\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Phase 2: Inference Start\")\n",
    "print()\n",
    "\n",
    "# 1. Step 1 --- for all the test logs, we detect their handover and store the results in a dictionary\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Step 1 --- handover detection (for all test logs) Start\")\n",
    "handover_dic = handover_detection_for_test_dic(df_dic_with_time_test, domain_1, lrc_model_1)\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Step 1 --- handover detection (for all test logs) Done\")\n",
    "print()\n",
    "\n",
    "# 2. Step 2 --- for all the test logs, we label the resources per case, and store the results in a file at a given path\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Step 2 --- resource labeling per case (for all test logs) Start\")\n",
    "create_event_df_after_step_two(df_dic_with_time_train, df_dic_with_time_test, handover_dic, domain_2, lrc_model_2, package_path)\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Step 2 --- resource labeling per case (for all test logs) Done\")\n",
    "print()\n",
    "\n",
    "# 3. Step 3 --- for each test event log in the test set, we apply simulated annealing to infer resources\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Step 3 --- resource assignment for logs Start\")\n",
    "for i in range(len(df_test_name_l)):\n",
    "    df_name = df_test_name_l[i]\n",
    "    data = df_dic_with_time_test[df_name][0]\n",
    "    print(f\"{datetime.now().strftime('%H:%M:%S')} {df_name} resource assignment for the log Start\")\n",
    "    case_ids, event_groups, target_event_l, target_case_l, case_event_normalize = assign_res_for_a_single_log(df_name, domain_3, domain_4, package_path)\n",
    "    best_assignments, best_cost = simulated_annealing_with_groups(case_ids, target_event_l, target_case_l, num_resources=num_resources, event_groups=event_groups, case_event_normalize=case_event_normalize, initial_temp=10000, cooling_rate=0.99, max_iter=1000000)\n",
    "    new_file_path = f\"{package_path}/{df_name}_final_TSI_resource_inferred_log.csv\"\n",
    "    log_name = f\"{df_name}_final_resource_inferred_log.csv\"\n",
    "    whether_TSI = True\n",
    "    add_resource_attr_to_csv(best_assignments, data, new_file_path, df_name, log_name, whether_TSI)\n",
    "    print(f\"{datetime.now().strftime('%H:%M:%S')} {df_name} resource assignment for the log Done\")\n",
    "    print(f\"{datetime.now().strftime('%H:%M:%S')} {df_name} resource inferred log evaluation Start\")\n",
    "    evaluation_pipeline(df_dic_with_time_test, df_name, case_ids, best_assignments)\n",
    "    print(f\"{datetime.now().strftime('%H:%M:%S')} {df_name} resource inferred log evaluation Done\")\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Step 3 --- resource assignment for logs Done\")\n",
    "\n",
    "print()\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Phase 2: Inference Done\")\n",
    "print()\n",
    "print(f\"{datetime.now().strftime('%H:%M:%S')} Three-step resource inference technique pipeline Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate logs with inferred resources using the following two baseline approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpic_2011 resource inference log with uniformly distribution start generating\n",
      "         Resource prediction accuracy: 0.0521\n",
      "         Event pair resource handover accuracy: 0.2273\n",
      "bpic_2011_final_uniformly_resource_inferred_log.csv stored successfully\n",
      "\n",
      "bpic_2011 resource inference log based on resource-activity distribution start generating\n",
      "         Resource prediction accuracy: 0.2157\n",
      "         Event pair resource handover accuracy: 0.2864\n",
      "bpic_2011_final_randomly_distributed_resource_inferred_log.csv stored successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this cell is related to baseline approaches (we choose two baseline approaches)\n",
    "# 1. randomly assigned\n",
    "# 2. assigned based on resource-activity number distributions\n",
    "\n",
    "# 1. randomly assigned\n",
    "\n",
    "def baseline_randomly_assign(case_ids, num_resources):\n",
    "    randomly_assign_res_l = [random.randint(0, num_resources-1) for _ in case_ids]\n",
    "    \n",
    "    return randomly_assign_res_l\n",
    "\n",
    "\n",
    "for i in range(len(df_test_name_l)):\n",
    "    df_name = df_test_name_l[i]\n",
    "    print(f\"{df_name} resource inference log with uniformly distribution start generating\")\n",
    "    data = df_dic_with_time_test[df_name][0]\n",
    "    case_ids, event_groups, target_event_l, target_case_l, case_event_normalize = assign_res_for_a_single_log(df_name, domain_3, domain_4, package_path)\n",
    "    randomly_res_l = baseline_randomly_assign(case_ids, num_resources=num_resources)\n",
    "    evaluation_pipeline(df_dic_with_time_test, df_name, case_ids, randomly_res_l)\n",
    "    new_file_path = f\"{package_path}/{df_name}_final_uniformly_resource_inferred_log.csv\"\n",
    "    log_name = f\"{df_name}_final_uniformly_resource_inferred_log.csv\"\n",
    "    add_resource_attr_to_csv(randomly_res_l, data, new_file_path, df_name, log_name)\n",
    "    print()\n",
    "\n",
    "\n",
    "# 2. assigned based on resource-activity number distributions\n",
    "def baseline_randomly_distribution_assign(case_ids, res_event_distri_l, num_resources):\n",
    "    population = list(range(num_resources))\n",
    "    weights = res_event_distri_l\n",
    "    random_distribution_assign_l = random.choices(population, weights=weights,k=len(case_ids))\n",
    "    \n",
    "    return random_distribution_assign_l\n",
    "\n",
    "\n",
    "for i in range(len(df_test_name_l)):\n",
    "    df_name = df_test_name_l[i]\n",
    "    print(f\"{df_name} resource inference log based on resource-activity distribution start generating\")\n",
    "    data = df_dic_with_time_test[df_name][0]\n",
    "    case_ids, event_groups, target_event_l, target_case_l, case_event_normalize = assign_res_for_a_single_log(df_name, domain_3, domain_4, package_path)\n",
    "    random_distribution_assign_l = baseline_randomly_distribution_assign(case_ids, target_event_l, num_resources=num_resources)\n",
    "    evaluation_pipeline(df_dic_with_time_test, df_name, case_ids, random_distribution_assign_l)\n",
    "    new_file_path = f\"{package_path}/{df_name}_final_randomly_distributed_resource_inferred_log.csv\"\n",
    "    log_name = f\"{df_name}_final_randomly_distributed_resource_inferred_log.csv\"\n",
    "    add_resource_attr_to_csv(random_distribution_assign_l, data, new_file_path, df_name, log_name)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
